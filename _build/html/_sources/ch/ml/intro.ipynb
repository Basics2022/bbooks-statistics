{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee40d70",
   "metadata": {},
   "source": [
    "(ml:intro)=\n",
    "# Introduction to Machine Learning\n",
    "\n",
    "Artificial intelligence can be broadly defined as a field dealing with making machines perform tasks that require intelligence, when performed by humans, like: reasoning, perception, representation, language processing, planning, learning\n",
    "\n",
    "**Machine learning** is a branch of AI focused on statistical algoritms that can **learn from data** and **generalize to unseen data** and perform tasks, without explicit instructions.[^ml-optimization]\n",
    "\n",
    "[^ml-optimization]: \"Without explicit instructions\" means that a systems has no user-coded behavior, but learns it usually via **optimization**, usually either involving minimization of an error function or maximization of an objective function or energy/information content.\n",
    "\n",
    "**Three core paradigms.** Algorithms in machine learning can be divided into three paradigms:\n",
    "\n",
    "- [**Supervised Learning, SL**](ml:sl): algorithm learns from labelled data; many applications can be reduced to 2 main tasks: **regression** (or function approximation) and **classification**.\n",
    "\n",
    "- [**Unsupervised Learning, UL**](ml:ul): algorithm learns pattern from un-labelled data; examples of taks in UL are clustering, dimensionality reduction (and recognition of *main* components in data), compression (retaining only relevant components in data). Some historical algorithms and linear algebra decompositions can be interpreted or generalized as unsupervised learning. \n",
    "\n",
    "- [**Reinforcement Learning, RL**](ml:rl)\n",
    "\n",
    "\n",
    "**Goals and methodology.** ML is mainly a engineering-oriented and an application-focused discipline, relying on statistical inference (**todo** *be more explicit*). Learning usually relies on **optimization**: performance of a ML system usually depends on a set of parameters $\\boldsymbol{\\theta}$, and a (much) smaller set of hyper-parameters, $\\boldsymbol{\\mu}$. Thus, learning usually results from the optimization of an objective function\n",
    "\n",
    "$$L(\\boldsymbol{\\theta}; \\boldsymbol{\\mu}) \\ ,$$\n",
    "\n",
    "w.r.t. parameters $\\boldsymbol{\\theta}$, whose value is learned/adjusted towards an optimal solution $\\boldsymbol{\\theta}^*$ that makes $L(\\boldsymbol{\\theta}^*; \\boldsymbol{\\mu})$ extreme. The choice of hyper-parameters $\\boldsymbol{\\mu}$ guides the training process and affect model behavior. Optimization usually relies on gradient methods, updating the parameters in the direction of the gradient of the objective function w.r.t. the parameters,\n",
    "\n",
    "$$\\boldsymbol{\\theta} \\ \\leftarrow \\ \\boldsymbol{\\theta} + \\alpha \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}; \\boldsymbol{\\mu}) \\ .$$\n",
    "\n",
    "<!--\n",
    "Recent development in hardware made feasible the design, construction and tuning of **large-dimensional** models, as the **multi-layered neural networks** used in **Deep learning**.[^deep-learning]\n",
    "-->\n",
    "\n",
    "Optimization of model parameters is made fast by the use of **back-propagation** and **automatic differentiation** (AD), which efficiently compute gradients of the cost function with respect to the modelâ€™s parameters, and technically feasible for large-dimensional models - as the ones used in multi-layered neural networks, in deep learning[^deep-learning] - by recent hardware improvement. These algorithms are not only feasible but also particularly well-suited (being a major driver for new designs) to modern processing architectures, such as **GPUs** and **TPUs**, that accelerate the large-scale matrix and tensor computations involved in both the forward and backward passes of training.\n",
    "\n",
    "[^deep-learning]: Deep learning can be roughly defined as that branch of machine learning using multi-layered neural networks, indeed.\n",
    "\n",
    "**todo** *Show NVIDIA, TSMC revenues*\n",
    "\n",
    "<!--\n",
    "\n",
    "| Year | Revenue ($M\\$$) |\n",
    "| ---- | --------------- |\n",
    "| 2013 |  4280           | \n",
    "| 2014 |  4130           |\n",
    "| 2015 |  4682           |\n",
    "| 2016 |  5010           |\n",
    "| 2017 |  6910           |\n",
    "| 2018 |  9714           |\n",
    "| 2019 | 11716           |\n",
    "| 2020 | 10918           |\n",
    "| 2021 | 16675           |\n",
    "| 2022 | 26914           |\n",
    "| 2033 | 26974           |\n",
    "| 2024 | 60922           |\n",
    "| 2025 |130497           |\n",
    "\n",
    "-->\n",
    "\n",
    "<!--\n",
    "```{code-cell} python\n",
    "import plotly.graphic_objects as go\n",
    "\n",
    "year = [ 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024 ]\n",
    "revs = [\n",
    "   4280, \n",
    "   4130,\n",
    "   4682,\n",
    "   5010,\n",
    "   6910,\n",
    "   9714,\n",
    "  11716,\n",
    "  10918,\n",
    "  16675,\n",
    "  26914,\n",
    "  26974,\n",
    "  60922,\n",
    " 130497\n",
    "]\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(x=year, y=revs)])\n",
    "fig.show()\n",
    "\n",
    "\n",
    "```\n",
    "-->\n",
    "\n",
    "---\n",
    "\n",
    "**todo** Add references: Bishop,..."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "dispaly_name": "Python 3(ipykernel)",
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}