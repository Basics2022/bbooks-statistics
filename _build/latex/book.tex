%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Introduction to Statistics}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{basics blog}
\date{Jun 30, 2025}
\release{}
\author{basics}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


\sphinxAtStartPar
This material is part of the \sphinxhref{https://basics2022.github.io/bbooks}{\sphinxstylestrong{basics\sphinxhyphen{}books project}}.

\sphinxAtStartPar
\sphinxstylestrong{Contents.}
\subsubsection*{Introduction to statistics}

\sphinxAtStartPar
Different approaches to statistics and \sphinxstylestrong{descriptive statistics}
\subsubsection*{Probability theory}
\subsubsection*{Inferential statistics}

\sphinxAtStartPar
Inferential and Bayesian statistics
\subsubsection*{Introduction to Machine Learning: SL, UL, ML}

\sphinxAtStartPar
Machine learning (ML) is a branch of artificial intelligence (AI) focused on designing systems that can learn from data to improve their performance on a task. ML frameworks include supervised learning (e.g., regression and classification), unsupervised learning (e.g., clustering, compression, principal component analysis), and reinforcement learning (e.g., planning and control). ML emphasizes practical problem\sphinxhyphen{}solving, grounded in statistical methods, numerical optimization, and enabled by advances in computing hardware.





\sphinxstepscope


\part{Introduction to Statistics}

\sphinxstepscope


\chapter{Introduction to Statistics}
\label{\detokenize{ch/intro/intro:introduction-to-statistics}}\label{\detokenize{ch/intro/intro:intro-intro}}\label{\detokenize{ch/intro/intro::doc}}
\sphinxstepscope


\chapter{Descriptive Statistics}
\label{\detokenize{ch/intro/descriptive:descriptive-statistics}}\label{\detokenize{ch/intro/descriptive:intro-descriptive}}\label{\detokenize{ch/intro/descriptive::doc}}
\sphinxstepscope


\part{Probability Theory}

\sphinxstepscope


\chapter{Introduction to probability theory}
\label{\detokenize{ch/prob/intro:introduction-to-probability-theory}}\label{\detokenize{ch/prob/intro:prob-intro}}\label{\detokenize{ch/prob/intro::doc}}
\sphinxAtStartPar
Probability theory is an axiomatic approach to probability, assigning
\subsubsection*{Stochastic variables}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Definition of stochastic variable

\item {} 
\sphinxAtStartPar
Discrete and continuous stochastic variables
\begin{itemize}
\item {} 
\sphinxAtStartPar
Probability functions, moments (if they exists, see heavy\sphinxhyphen{}tailed distribution), and examples

\end{itemize}

\item {} 
\sphinxAtStartPar
Multi\sphinxhyphen{}dimensional stochastic variables:
\begin{itemize}
\item {} 
\sphinxAtStartPar
joint, conditional, marginal probability

\item {} 
\sphinxAtStartPar
Bayes’ theorem

\item {} 
\sphinxAtStartPar
independence

\item {} 
\sphinxAtStartPar
moments: covariance, correlation

\end{itemize}

\item {} 
\sphinxAtStartPar
Generators…

\item {} 
\sphinxAtStartPar
I.i.d. variables: law of large numbers, central limit theorem; convergence of statistics (reference to measure in the definition of a sthocastic variable)

\item {} 
\sphinxAtStartPar
Sampling

\item {} 
\sphinxAtStartPar
Extra:
\begin{itemize}
\item {} 
\sphinxAtStartPar
heavy tails probability functions

\end{itemize}

\end{itemize}
\subsubsection*{Stochastic processes}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Definition of stochastic process

\item {} 
\sphinxAtStartPar
Time\sphinxhyphen{}continuous/time\sphinxhyphen{}discrete

\item {} 
\sphinxAtStartPar
Ergodicity and stationariety:
\begin{itemize}
\item {} 
\sphinxAtStartPar
moments, correlation,…

\item {} 
\sphinxAtStartPar
analysis in time and Fourier domains of time\sphinxhyphen{}signals

\end{itemize}

\item {} 
\sphinxAtStartPar
Applications:
\begin{itemize}
\item {} 
\sphinxAtStartPar
example of processes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
white noise

\item {} 
\sphinxAtStartPar
Wiener process (Brownian motion): definition, application, relation with

\item {} 
\sphinxAtStartPar
discrete\sphinxhyphen{}time Markov process (useful in RL, can be interpreted as a discretized continuous process)

\end{itemize}

\item {} 
\sphinxAtStartPar
response of LTI to random input

\end{itemize}

\end{itemize}
\subsubsection*{Stochastic fields}

\sphinxstepscope


\section{Definition of stochastic variable}
\label{\detokenize{ch/prob/rv-def:definition-of-stochastic-variable}}\label{\detokenize{ch/prob/rv-def:prob-rv-def}}\label{\detokenize{ch/prob/rv-def::doc}}
\sphinxAtStartPar
In this section, first a formal definition of a stochastic (or random) variable is provided and then discussed. While the definition may initially appear abstract or esoteric, but their nothing more than an extension of set theory. Understanding concepts such as \sphinxstyleemphasis{probability space}, \sphinxstyleemphasis{\(\sigma\)\sphinxhyphen{}algebra}, \sphinxstyleemphasis{measure}, opens the path to a rigorous approach to probability theory.
\label{ch/prob/rv-def:def:rv}
\begin{sphinxadmonition}{note}{Definition 3.1.1 (Random variable)}



\sphinxAtStartPar
Given a \sphinxstylestrong{probability space} \((\Omega, \mathcal{F}, \nu)\) and a \sphinxstylestrong{measurable space} \((E, \mathcal{E})\), a \sphinxstylestrong{random variable} is a measurable function, \(X: \Omega \ \rightarrow \ E\).
\end{sphinxadmonition}

\sphinxAtStartPar
Here, the set \(\Omega\) is usually defined as the \sphinxstylestrong{event set}, \(\mathcal{F}\) is a \sphinxstylestrong{\(\sigma\)\sphinxhyphen{}algebra} on \(\Omega\), \(\nu\) is a \sphinxstylestrong{probability measure}; the set \(E\) is usually defined as the \sphinxstylestrong{set of possible outcomes}, and \(\mathcal{E}\) is a \sphinxstylestrong{\(\sigma\)\sphinxhyphen{}algebra} on it.

\sphinxAtStartPar
Ok, let’s explain now every concept appearing in the definition of a random variable.
\label{ch/prob/rv-def:def:sigma-algebra}
\begin{sphinxadmonition}{note}{Definition 3.1.2 (\protect\(\sigma\protect\)\sphinxhyphen{}algebra)}



\sphinxAtStartPar
A \(\sigma\)\sphinxhyphen{}algebra \(\mathcal{F}\) on a set \(\Omega\), a \(\sigma\)\sphinxhyphen{}algebra is a family of subsets of \(\Omega\) that satisfies some properties:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\Omega\) is in the \(\sigma\)\sphinxhyphen{}algebra

\item {} 
\sphinxAtStartPar
closure under complementation. Taking any subset of \(\Omega\) that belongs to the \(\sigma\)\sphinxhyphen{}algebra, its complent belongs to the \(\sigma\)\sphinxhyphen{}algebra as well

\item {} 
\sphinxAtStartPar
closure under countable union. A countable (that can be indexed by integer numbers) union of elements in the \(\sigma\)\sphinxhyphen{}algebra defines an element that belongs to the \(\sigma\)\sphinxhyphen{}algebra

\end{enumerate}
\end{sphinxadmonition}
\label{ch/prob/rv-def:def:measurable-space}
\begin{sphinxadmonition}{note}{Definition 3.1.3 (Measurable space \protect\( (\Omega, \mathcal{F})\protect\))}


\end{sphinxadmonition}
\label{ch/prob/rv-def:def:prob-measure}
\begin{sphinxadmonition}{note}{Definition 3.1.4 (Probability measure \protect\(\nu\protect\))}


\end{sphinxadmonition}
\label{ch/prob/rv-def:def:prob-space}
\begin{sphinxadmonition}{note}{Definition 3.1.5 (Probability space \protect\( (\Omega, \mathcal{F}, \nu)\protect\))}



\sphinxAtStartPar
If \((\Omega, \mathcal{F})\) is a measureble space and \(\nu\) is a probability measure, then the triplet \((\Omega, \mathcal{F}, \nu)\) is a probability space.
\end{sphinxadmonition}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
Event space not coinciding with the power set of events

\sphinxAtStartPar
If event space \(\Omega\) doesn’t coincide with the power set but it’s a subset of it, then it’s not guaranteed that such an event space can be used to define a \(\sigma\)\sphinxhyphen{}algebra.
\end{sphinxadmonition}

\sphinxstepscope


\section{Discrete stochastic variables}
\label{\detokenize{ch/prob/rv-discrete:discrete-stochastic-variables}}\label{\detokenize{ch/prob/rv-discrete:prob-rv-discrete}}\label{\detokenize{ch/prob/rv-discrete::doc}}
\sphinxstepscope


\section{Continuous stochastic variables}
\label{\detokenize{ch/prob/rv-continuous:continuous-stochastic-variables}}\label{\detokenize{ch/prob/rv-continuous:prob-rv-continuous}}\label{\detokenize{ch/prob/rv-continuous::doc}}

\subsection{Examples}
\label{\detokenize{ch/prob/rv-continuous:examples}}\label{\detokenize{ch/prob/rv-continuous:prob-rv-continuous-ex}}
\sphinxAtStartPar
Here some common examples of continuous random variables are introduced. Their functional dependence on the value of the r.v. is quite easy to remember, while the normalization factor could look quite “esoteric”.


\subsubsection{Normal distribution, \protect\(\mathscr{N}(\mu, \sigma^2)\protect\)}
\label{\detokenize{ch/prob/rv-continuous:normal-distribution-mathscr-n-mu-sigma-2}}\label{\detokenize{ch/prob/rv-continuous:prob-rv-continuous-ex-normal}}
\sphinxAtStartPar
pdf is
\begin{equation*}
\begin{split}f(x; \mu, \sigma^2) = \dfrac{1}{\sqrt{2 \pi \sigma^2}}  e^{-\frac{(x-\mu)^2}{2 \sigma^2}}  \quad \propto \quad e^{- \frac{(x-\mu)^2}{2 \sigma^2}}\end{split}
\end{equation*}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Moment
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Value
\\
\hline
\sphinxAtStartPar
Expected value
&
\sphinxAtStartPar
\(\mu\)
\\
\hline
\sphinxAtStartPar
Variance
&
\sphinxAtStartPar
\(\sigma^2\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Expected value, \(\mu\); variance, \(\sigma^2\).
\subsubsection*{Unitariety}
\begin{equation*}
\begin{split}\int_{x=-\infty}^{+\infty} e^{-\frac{(x-\mu)^2}{2 \sigma^2}} = \sqrt{2 \pi \sigma^2}\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{todo} \sphinxstyleemphasis{integral \(\int_{-\infty}^{+\infty} e^{- \alpha x^2} dx\)}
\subsubsection*{Expected value}
\begin{equation*}
\begin{split}\begin{aligned}
  \mathbb{E}\left[ X \right] 
  & =  \int_{x = -\infty}^{+\infty} x \dfrac{1}{\sqrt{2 \pi \sigma^2}}  e^{-\frac{(x-\mu)^2}{2 \sigma^2}}   \\
\end{aligned}\end{split}
\end{equation*}\subsubsection*{Variance}
\begin{equation*}
\begin{split}\begin{aligned}
  \mathbb{E}\left[ (X-\mu)^2 \right] 
  & =  \dots
\end{aligned}\end{split}
\end{equation*}

\subsubsection{Chi\sphinxhyphen{}square, \protect\(\chi_N^2\protect\)}
\label{\detokenize{ch/prob/rv-continuous:chi-square-chi-n-2}}\label{\detokenize{ch/prob/rv-continuous:prob-rv-continuous-ex-chi-2}}\begin{equation*}
\begin{split}\chi_N^2 := \sum_{n=1}^N X^2_n\end{split}
\end{equation*}
\sphinxAtStartPar
pdf is
\begin{equation*}
\begin{split}f(x; n) = \dots \quad \propto \quad  x^{\frac{n}{2} - 1} e^{- \frac{x}{2}} \end{split}
\end{equation*}

\subsubsection{Student\sphinxhyphen{}\protect\(t\protect\) distribution, \protect\(t_{\nu}\protect\)}
\label{\detokenize{ch/prob/rv-continuous:student-t-distribution-t-nu}}\label{\detokenize{ch/prob/rv-continuous:prob-rv-continuous-ex-t-student}}\begin{equation*}
\begin{split}t_{\nu} = \dfrac{Z}{\sqrt{\dfrac{K}{\nu}}} \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
with \(Z \sim \mathscr{N}(0,1)\), and \(K \sim \chi^2_\nu\).

\sphinxAtStartPar
pdf is
\begin{equation*}
\begin{split}f(x; n) = \dots \quad \propto \quad \left( 1 + \frac{x^2}{n} \right)^{-\frac{n+1}{2}} \end{split}
\end{equation*}
\sphinxstepscope


\section{Multi\sphinxhyphen{}dimensional stochastic variables}
\label{\detokenize{ch/prob/rv-multi-dimensional:multi-dimensional-stochastic-variables}}\label{\detokenize{ch/prob/rv-multi-dimensional:prob-multidim}}\label{\detokenize{ch/prob/rv-multi-dimensional::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
joint distribution
\begin{equation*}
\begin{split}p_{XY}(x,y) \end{split}
\end{equation*}
\item {} 
\sphinxAtStartPar
marginal distribution. For continuous variables
\begin{equation*}
\begin{split}p_X(x) := \int_{y} p_{XY}(x,y) \, dy\end{split}
\end{equation*}
\sphinxAtStartPar
while for discrete variables
\begin{equation*}
\begin{split}p_X(x_i) = \sum_j p_{XY}(x_i,y_j)\end{split}
\end{equation*}
\item {} 
\sphinxAtStartPar
conditional distribution, \(p_{X|Y}(x|y)\). The following holds
\begin{equation*}
\begin{split}p_{XY} = p_{X|Y} \, p_Y = p_{Y|X} p_X\end{split}
\end{equation*}
\end{itemize}

\sphinxAtStartPar
For continuous r.v., integrating over \(x\) the relation \(p(x,y) = p(x|y) p(y)\)
\begin{equation*}
\begin{split}\begin{aligned}
  \int_{x} p(x,y) d x = \int_{x} p(x|y) \, p(y) \, dx = p(y) \underbrace{\int_{x} p(x|y) \, dx}_{= 1} = p(y) \ ,
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
as the normalization condition holds for conditional distribution \(p(x|y)\).
\label{ch/prob/rv-multi-dimensional:property-0}
\begin{sphinxadmonition}{note}{Property 3.4.1}


\begin{equation*}
\begin{split}\begin{aligned}
  p(i,j) = p(i|j) p(j)
\end{aligned}\end{split}
\end{equation*}\begin{equation*}
\begin{split}\sum_i p(i,j) = \underbrace{\sum_i p(i|j)}_{=1} p(j) = p(j)\end{split}
\end{equation*}\end{sphinxadmonition}


\subsection{Moments}
\label{\detokenize{ch/prob/rv-multi-dimensional:moments}}\label{\detokenize{ch/prob/rv-multi-dimensional:prb-multidim-moments}}\begin{itemize}
\item {} 
\sphinxAtStartPar
expected value
\begin{equation*}
\begin{split}\boldsymbol{\mu}_{\mathbf{X}} := \mathbb{E}\left[ \mathbf{X} \right] = \int_{\mathbf{x}} p(\mathbf{x}) \, \mathbf{x} \,  d \mathbf{x}\end{split}
\end{equation*}
\item {} 
\sphinxAtStartPar
covariance
\begin{equation*}
\begin{split}\boldsymbol{\sigma}^2_{\mathbf{X}} := \mathbb{E} \left[ \Delta \mathbf{X} \, \Delta \mathbf{X}^T \right] = \int_{\mathbf{x}} p(\mathbf{x}) \, \Delta \mathbf{x} \Delta \mathbf{x}^T \, d \mathbf{x} \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\Delta \mathbf{X} := \mathbf{X} - \boldsymbol{\mu}_{\mathbf{X}} \), and \(\Delta \mathbf{x} = \mathbf{x} - \boldsymbol{\mu}_{\mathbf{X}}\).

\sphinxAtStartPar
Taking a pair of components \(X_i\), \(X_j\) of the random vector \(\mathbf{X}\), their covariance is the \(ij\) component of the array \(\boldsymbol{\sigma}^2\),
\begin{equation*}
\begin{split}\sigma^2_{ij} := \mathbb{E}\left[ \Delta X_i \, \Delta X_j \right] =: \rho_{ij} \sigma_i \sigma_j \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
having introduced \sphinxstylestrong{(Pearson) correlation}, \(\rho_{ij}\), between random variable \(X_i\) and \(X_j\), and being \(\sigma_i\) the standard deviation of variable \(X_i\), square root of its variance \(\sigma^2_i\),
\begin{equation*}
\begin{split}\begin{aligned}
    \sigma^2_i 
    & = \mathbb{E}\left[ \left( X_i - \mu_i \right)^2  \right] = \\
    & = \int_{\mathbf{x}} (x_i - \mu_i)^2 p_{\mathbf{X}}(\mathbf{x}) d \mathbf{x} = \\
    & = \int_{x_i} (x_i - \mu_i)^2 p_i (x_i) \, d x_i 
  \end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
Here the integrals read
\begin{equation*}
\begin{split}\begin{aligned}
    \mu_i 
    & = \int_{\mathbf{x}} x_i \, p_{\mathbf{X}}(\mathbf{x}) x_i \, d \mathbf{x} = \\
    & = \int_{\mathbf{x}} x_i \, p(x_1, x_2, \dots, x_i, \dots, x_n) d x_1 d x_2 \dots d x_i \dots d x_n = \\
    & = \int_{\mathbf{x}} x_i \, p(x_i) p(x_1, x_2, \dots, x_{i-1}, x_{i+1}, \dots, x_n | x_i) d x_1 d x_2 \dots d x_i \dots d x_n = \\
    & = \int_{x_i} x_i \, p(x_i) \underbrace{\int_{x_1} \dots \int_{x_n} p(x_1, x_2, \dots, x_{i-1}, x_{i+1}, \dots, x_n | x_i) d x_1 \dots d x_{i-1} d x_{i+1} \dots d x_n}_{= 1 \text{ $\forall x_i$}} d x_i = \\
    & = \int_{x_i} x_i \, p(x_i) \, d x_i \ .
  \end{aligned}\end{split}
\end{equation*}
\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Property of correlation.} \(|\rho_{XY}| \le 1\). Proof with Cauchy\sphinxhyphen{}Schwartz inequality \sphinxstylestrong{todo}

\begin{sphinxadmonition}{note}{Notation}

\sphinxAtStartPar
Here, covariance is indicated as \(\boldsymbol{\sigma}^2\). This is not a power \(2\), but just a symbol, at most recalling that covariance matrix is \sphinxstylestrong{semi\sphinxhyphen{}definite positive}.
\end{sphinxadmonition}

\sphinxAtStartPar
\sphinxstylestrong{Properties of covariance.}
\begin{itemize}
\item {} 
\sphinxAtStartPar
symmetric

\item {} 
\sphinxAtStartPar
semi\sphinxhyphen{}definite positive

\item {} 
\sphinxAtStartPar
spectrum…

\end{itemize}


\subsection{Bayes’ theorem}
\label{\detokenize{ch/prob/rv-multi-dimensional:bayes-theorem}}\label{\detokenize{ch/prob/rv-multi-dimensional:prob-multidim-bayes}}\label{ch/prob/rv-multi-dimensional:theorem-1}
\begin{sphinxadmonition}{note}{Theorem 3.4.1 (Bayes’ theorem)}



\sphinxAtStartPar
Where \(p_Y(y) \ne 0\),
\begin{equation*}
\begin{split}p_{X|Y}(x|y) = \dfrac{p_{XY}(x,y)}{p_Y(y)}\end{split}
\end{equation*}\end{sphinxadmonition}


\subsection{Statistical independence}
\label{\detokenize{ch/prob/rv-multi-dimensional:statistical-independence}}\label{\detokenize{ch/prob/rv-multi-dimensional:prob-multidim-independence}}\label{ch/prob/rv-multi-dimensional:definition-2}
\begin{sphinxadmonition}{note}{Definition 3.4.1 (Independent random variables)}



\sphinxAtStartPar
Given two random variables \(X\), \(Y\) with joint distribution, the random variable \(X\) is independent from \(Y\) if its conditional probability equals its marginal probability,
\begin{equation*}
\begin{split}p_{X|Y} = p_X \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
i.e. the probability of \(X\) doesn’t depend on \(Y\).
\end{sphinxadmonition}


\subsubsection{Independence implies no correlation}
\label{\detokenize{ch/prob/rv-multi-dimensional:independence-implies-no-correlation}}\label{\detokenize{ch/prob/rv-multi-dimensional:prob-multidim-independence-no-correlation}}
\sphinxAtStartPar
Given two random variables \(X\), \(Y\) are independent if \(p(x|y) = p(x)\) and thus \(p(x,y) = p(x) p(y)\). Covariance of two random variable reads
\begin{equation*}
\begin{split}\sigma^2_{xy} = \mathbb{E} \left[ (X - \mu_X) (Y - \mu_Y)  \right] \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
and if they’re independent, it immediately follows that their covariance \(\sigma^2_{XY}\) is zero (and so their correlation \(\rho_{XY}\))
\begin{equation*}
\begin{split}\sigma^2_{xy} = \underbrace{\mathbb{E} \left[ X - \mu_X \right]}_{=0} \underbrace{\mathbb{E} \left[ Y - \mu_Y \right]}_{=0} = 0 \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
as the expected value of the deviation from the expected value is zero, \(\mathbb{E} \left[ X - \mathbb{E}[X] \right] = 0\).
\subsubsection*{Proof for continuous r.v.}
\begin{equation*}
\begin{split}\begin{aligned}
  \sigma^2_{xy}
  & = \mathbb{E} \left[ (X - \mu_X) (Y - \mu_Y)  \right] = \\
  & = \int_{x,y} ( x - \mu_X) ( y - \mu_Y) p(x,y) \, dx dy = && (1) \\
  & = \int_{x,y} ( x - \mu_X) ( y - \mu_Y) p(x) p(y) \, dx dy = \\
  & = \int_{x} ( x - \mu_X) p(x) dx \, \int_{y} ( y - \mu_Y) p(y) dy = && (2) \\
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
having used here the common notation abuse \(p_X(x) = p(x)\) and \((1)\) statistical independence, \(p(x,y) = p(x) p(y)\), and \((2)\) \(\mathbb{E}\left[ X - \mathbb{E}[X] \right] = 0\).
\subsubsection*{Proof for discrete r.v.}

\sphinxAtStartPar
Repeat the proof for continuous r.v. using summations instead of integrals.

\sphinxstepscope


\section{Transformations of probability functions}
\label{\detokenize{ch/prob/transformations:transformations-of-probability-functions}}\label{\detokenize{ch/prob/transformations:prob-transformations}}\label{\detokenize{ch/prob/transformations::doc}}
\sphinxstepscope


\section{Characteristic functions}
\label{\detokenize{ch/prob/characteristic-fun:characteristic-functions}}\label{\detokenize{ch/prob/characteristic-fun:prob-characteristic-fun}}\label{\detokenize{ch/prob/characteristic-fun::doc}}
\sphinxAtStartPar
Characteristic function of a random variable \(X\) is defined as
\begin{equation*}
\begin{split}\varphi_X(t) := \mathbb{E} \left[ e^{i t X} \right] \ .\end{split}
\end{equation*}
\sphinxAtStartPar
Characteristic function of a continuous random variable with proabibility density function \(f(x)\) thus reads
\begin{equation*}
\begin{split}\varphi_X(t) := \mathbb{E} \left[ e^{i t X} \right] = \int_{x \in D_x} f(x) e^{i t x} \, dx \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
i.e. its the \sphinxhref{https://basics2022.github.io/bbooks-math-miscellanea/ch/complex/fourier-transform.html\#fourier-transform}{\sphinxstylestrong{Fourier transform}} of its pdf.
\label{ch/prob/characteristic-fun:ex:char-fun:multidimensional}
\begin{sphinxadmonition}{note}{Example 3.6.1 (Characteristic function of a multi\sphinxhyphen{}dimensional variable)}


\begin{equation*}
\begin{split}Z(\mathbf{Y})\end{split}
\end{equation*}\begin{equation*}
\begin{split}\varphi_{Z(\mathbf{Y})} := \mathbb{E}\left[ e^{i t Z(\mathbf{Y})} \right] = \int_{\mathbf{y}} e^{i t Z(\mathbf{y})} f(\mathbf{y}) \, d \mathbf{y}\end{split}
\end{equation*}\end{sphinxadmonition}
\label{ch/prob/characteristic-fun:ex:char-fun:independent}
\begin{sphinxadmonition}{note}{Example 3.6.2 (Characteristic function of a linear combination of independent variables)}


\begin{equation*}
\begin{split}Z(\mathbf{Y}) = a_1 Y_1 + \dots a_n Y_n \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
with
\begin{equation*}
\begin{split}f(\mathbf{y}) = f(y_1, \dots, y_n) =  f_1(y_1) \dots f_n(y_n) \ .\end{split}
\end{equation*}\begin{equation*}
\begin{split}\begin{aligned}
  \varphi_{Z(\mathbf{Y})} 
  & := \mathbb{E}\left[ e^{i t Z(\mathbf{Y})} \right] = \\
  & = \int_{\mathbf{y}} e^{i t \left( \sum_k a_k y_k \right)} f(\mathbf{y}) \, d \mathbf{y} = \\
  & = \int_{y_1} e^{i t a_1 y_1} f_1(y_1) \, d y_1 \, \dots \int_{y_n} e^{i t a_n y_n} f_n(y_n) \, d y_n = \\
  & = \varphi_{Y_1}(a_1 t) \dots \varphi_{Y_n}(a_n t) \ . 
\end{aligned}\end{split}
\end{equation*}\end{sphinxadmonition}
\label{ch/prob/characteristic-fun:ex:char-fun:taylor}
\begin{sphinxadmonition}{note}{Example 3.6.3 (Taylor expansion of characteristic function)}



\sphinxAtStartPar
For “small” values of \(t\), an approximation of the characteristic function is provided by Taylor expansion around \(t=0\),
\begin{equation*}
\begin{split}\begin{aligned}
\int e^{ i y t } f(y) \, dy 
& = \int \left[ 1 + i y t - \dfrac{1}{2} (yt)^2 + o(t^2) \right] f(y) dy = && (1) \\
& = 1 + i \mu t - \dfrac{1}{2} t^2 \left( \sigma^2 + \mu^2 \right) + o(t^2)
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
as \((1)\) \(\sigma^2 = \mathbb{E}[(y - \mu)^2] = \mathbb{E}[y^2] - \mu^2\)
\end{sphinxadmonition}
\label{ch/prob/characteristic-fun:ex:char-fun:normal}
\begin{sphinxadmonition}{note}{Example 3.6.4 (Characteristic function of a normal distribution \protect\(\mathscr{N}(0,1)\protect\))}


\begin{equation*}
\begin{split}f(x) = \dfrac{1}{\sqrt{2 \pi}} \exp\left( -\dfrac{x^2}{2} \right)\end{split}
\end{equation*}\begin{equation*}
\begin{split}\begin{aligned}
\int_{x=-\infty}^{+\infty} e^{ i x t } f(x) \, dx
& = \dfrac{1}{\sqrt{2 \pi}} \int_{x=-\infty}^{+\infty} e^{ i x t - \frac{x^2}{2} }  \, dx = && (1)  \\
& = \dfrac{1}{\sqrt{2 \pi}} \int_{x=-\infty}^{+\infty} e^{ - \frac{(x-it)^2}{2} }  \, dx \, e^{-\frac{t^2}{2}} = && (2)  \\
& = \dfrac{1}{\sqrt{2 \pi}} \, \sqrt{2 \pi} \, e^{-\frac{t^2}{2}} = e^{-\frac{t^2}{2}}  \\
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
having \((1)\) completed the square \((x - it)^2 = x^2 - i 2 x t - t^2\), and evaluated the integral \sphinxstylestrong{todo} (it’s similar to the standard result \(\int_{-\infty}^{+\infty} e^{x^2} \, dx = \sqrt{2 \pi}\), but with complex variable. Link to math material, complex calculus).
\end{sphinxadmonition}

\sphinxstepscope


\section{Convergence in statistics}
\label{\detokenize{ch/prob/convergence:convergence-in-statistics}}\label{\detokenize{ch/prob/convergence:prob-convergence}}\label{\detokenize{ch/prob/convergence::doc}}

\subsection{Convergence in distribution \sphinxhyphen{} weak convergence}
\label{\detokenize{ch/prob/convergence:convergence-in-distribution-weak-convergence}}\label{\detokenize{ch/prob/convergence:prob-convergence-weak}}
\sphinxAtStartPar
A sequence of \(X_i\) of real\sphinxhyphen{}valued random variables, cumulative distribution functions \(F_i\), converges in distribution to a random variable \(X\) with cumulative distribution \(F\) is
\begin{equation*}
\begin{split}\lim_{n \rightarrow +\infty} F_n(x) = F(x) \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
for \(\forall x \in \mathbb{R}\) where \(F(x)\) is continuous.

\sphinxAtStartPar
For multi\sphinxhyphen{}valued random variables, the condition reads
\begin{equation*}
\begin{split}\lim_{n \rightarrow +\infty} P( X_n \in A) = P( X \in A) \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
for every \(A \subset \mathbb{R}^n\) …\sphinxstylestrong{todo}


\subsection{Convergence in probability}
\label{\detokenize{ch/prob/convergence:convergence-in-probability}}\label{\detokenize{ch/prob/convergence:prob-convergence-probability}}\begin{equation*}
\begin{split}\lim_{n \rightarrow + \infty} P \left(\left| X_n - X \right| > \varepsilon  \right) = 0\end{split}
\end{equation*}
\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
Convergence in probability and convergence in distribution

\sphinxAtStartPar
Convergence in probability \( \rightarrow \) convergence in distribution, but not viceversa.

\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Convergence\_of\_random\_variables\#Counterexamples}{Example taken from wikipedia}
\end{sphinxadmonition}


\subsection{Almost sure convergence \sphinxhyphen{} strong convergence}
\label{\detokenize{ch/prob/convergence:almost-sure-convergence-strong-convergence}}\label{\detokenize{ch/prob/convergence:prob-convergence-strong}}\begin{equation*}
\begin{split}P\left( \lim_{n \rightarrow +\infty} X_n = X \right) = 1\end{split}
\end{equation*}
\sphinxAtStartPar
i.e. events for which \(X_n\) doesn’t converge to \(X\) has probability \(0\),
\begin{equation*}
\begin{split}P\left( \omega \in \Omega: \ \lim_{n \rightarrow + \infty} X_n(\omega) = X(\omega) \right) = 1\end{split}
\end{equation*}

\subsection{Sure convergence \sphinxhyphen{} pointwise convergence}
\label{\detokenize{ch/prob/convergence:sure-convergence-pointwise-convergence}}\label{\detokenize{ch/prob/convergence:prob-convergence-point}}\begin{equation*}
\begin{split}\left\{ \omega \in \Omega: \ \lim_{n \rightarrow + \infty} X_n(\omega) = X(\omega) \right\} = \Omega \ .\end{split}
\end{equation*}
\sphinxAtStartPar
The same definition of almost sure convergence, without allowing the existance of sets with zero probability where convergence is not satisfied. Thus, it’s likely there is no point in using sure converence instead of almost sure convergence in proability theory.


\subsection{Convergence in absolute moments: mean,…}
\label{\detokenize{ch/prob/convergence:convergence-in-absolute-moments-mean}}\label{\detokenize{ch/prob/convergence:prob-convergence-mean}}\begin{equation*}
\begin{split}\lim_{n \rightarrow +\infty} \mathbb{E}\left( \left| X_n - X \right|^r \right) = 0\end{split}
\end{equation*}
\sphinxstepscope


\section{Independent identically distributed random variables}
\label{\detokenize{ch/prob/iid:independent-identically-distributed-random-variables}}\label{\detokenize{ch/prob/iid:prob-iid}}\label{\detokenize{ch/prob/iid::doc}}\label{ch/prob/iid:definition-0}
\begin{sphinxadmonition}{note}{Definition 3.8.1 (Independent identically distributed (iid) random variables)}


\end{sphinxadmonition}


\subsection{Law of the large numbers}
\label{\detokenize{ch/prob/iid:law-of-the-large-numbers}}\label{\detokenize{ch/prob/iid:prob-iid-large-numbers}}

\subsubsection{Weak form}
\label{\detokenize{ch/prob/iid:weak-form}}\label{\detokenize{ch/prob/iid:prob-iid-large-numbers-weak}}
\sphinxAtStartPar
\sphinxstylestrong{todo}


\subsubsection{Strong form}
\label{\detokenize{ch/prob/iid:strong-form}}\label{\detokenize{ch/prob/iid:prob-iid-large-numbers-strong}}
\sphinxAtStartPar
\sphinxstylestrong{todo}


\subsection{Central Limit Theorem}
\label{\detokenize{ch/prob/iid:central-limit-theorem}}\label{\detokenize{ch/prob/iid:prob-iid-clt}}\label{ch/prob/iid:thm:clt}
\begin{sphinxadmonition}{note}{Theorem 3.8.1 (CLT)}



\sphinxAtStartPar
Let \(\{ X_k \}_{k=1:n}\) a sequence of iid random variables with average value \(\mathbb{E}[X_k] = \mu\) and \sphinxstylestrong{finite}%
\begin{footnote}[1]\sphinxAtStartFootnote
Does the CLT hold for {\hyperref[\detokenize{ch/prob/heavy-tailed:prob-heavy-tailed}]{\sphinxcrossref{\DUrole{std,std-ref}{\sphinxstylestrong{heavy\sphinxhyphen{}tailed} distributions}}}}?
%
\end{footnote} variance \(\mathbb{E}[(X_k-\mu)^2] = \sigma^2 < \infty\), then the \sphinxstylestrong{sample average}
\begin{equation*}
\begin{split}\overline{X}_n := \dfrac{1}{n} \sum_{k=1}^n X_k \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
{\hyperref[\detokenize{ch/prob/convergence:prob-convergence-weak}]{\sphinxcrossref{\DUrole{std,std-ref}{converges in distribution}}}} \sphinxhyphen{} or weakly converges \sphinxhyphen{} to the normal distribution \(\mathscr{N}\left(\mu, \frac{\sigma^2}{n} \right)\),
\begin{equation*}
\begin{split}\overline{X}_n \quad \rightarrow^d \quad \mathscr{N}\left(\mu, \frac{\sigma^2}{n} \right) \ .\end{split}
\end{equation*}\end{sphinxadmonition}
\subsubsection*{Proof of CLT}

\sphinxAtStartPar
Let \(\{ X_k \}_{k=1:n}\) the sequence of iid random variables. Thus, \(\sum_{k=1}^n X_k\) has expected value \(n \mu\) and variance \(n \sigma^2\). Let
\begin{equation*}
\begin{split}Z_n := \dfrac{\sum_{k=1}^n X_k - n \mu}{\sqrt{n \sigma^2}} = \sum_{k=1}^{n} \dfrac{X_k - \mu}{\sqrt{n \sigma^2}} =: \sum_{k=1}^n \dfrac{Y_k}{\sqrt{n}} \ .\end{split}
\end{equation*}
\sphinxAtStartPar
Expeceted value and variance of variables \(Y_k\) are respectively \(\mathbb{E}[Y_k] = 0\) and \(\mathbb{E}[Y_k^2] = 1\).
The {\hyperref[\detokenize{ch/prob/characteristic-fun:prob-characteristic-fun}]{\sphinxcrossref{\DUrole{std,std-ref}{characteristic function}}}} of \(Z_n\), see {\hyperref[\detokenize{ch/prob/characteristic-fun:ex:char-fun:independent}]{\sphinxcrossref{Example 3.6.2}}} for the linear combination of independent variables, reads
\begin{equation*}
\begin{split}\begin{aligned}
  \varphi_{Z_n}(t) 
  & = \varphi_{Y_1} \left( \dfrac{t}{\sqrt{n}} \right) \dots \varphi_{Y_n} \left( \dfrac{t}{\sqrt{n}} \right) = &&  (1) \\
  & = \left[ \varphi_{Y_1} \left( \dfrac{t}{\sqrt{n}} \right) \right]^n \ ,
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
as \((1)\) the variables are not only independent but identically distributed: as they have the same pdf, they also have the same characteristic function. Expanding in Taylor series, see example {\hyperref[\detokenize{ch/prob/characteristic-fun:ex:char-fun:taylor}]{\sphinxcrossref{Example 3.6.3}}} for \(\frac{t}{\sqrt{n}} \rightarrow 0\), the approximation of the characteristic function reads (remembering that \(Y_n\) have zero expected value and unit variance),
\begin{equation*}
\begin{split}\varphi_{Y_1}\left( \dfrac{t}{\sqrt{n}} \right) \sim 1 - \dfrac{t^2}{2 n} \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
while
\begin{equation*}
\begin{split}\varphi_{Z_n}(t) = \left[ \varphi_{Y_1} \left( \dfrac{t}{\sqrt{n}} \right) \right]^n \sim \left[  1 - \dfrac{t^2}{2 n}  \right]^n \sim e^{- \frac{t^2}{2}} \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
i.e. it converges to the characteristic function of a normal distribution \(\mathscr{N}(0,1)\), see {\hyperref[\detokenize{ch/prob/characteristic-fun:ex:char-fun:normal}]{\sphinxcrossref{Example 3.6.4}}}.

\sphinxAtStartPar
\sphinxstylestrong{Levy’s continuity theorem} completes the proof. \sphinxstylestrong{todo}


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Heavy\sphinxhyphen{}tailed distributions}
\label{\detokenize{ch/prob/heavy-tailed:heavy-tailed-distributions}}\label{\detokenize{ch/prob/heavy-tailed:prob-heavy-tailed}}\label{\detokenize{ch/prob/heavy-tailed::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Does the {\hyperref[\detokenize{ch/prob/iid:prob-iid-clt}]{\sphinxcrossref{\DUrole{std,std-ref}{CLT}}}} hold for heavy\sphinxhyphen{}tailed distributions?

\end{itemize}


\subsection{References}
\label{\detokenize{ch/prob/heavy-tailed:references}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://aaronclauset.github.io/rareevents/}{Clauset, Aaron and Woodard, Ryan, “Estimating the historical and future probabilities of large terrorist events.” Annals of Applied Statistics 7(4), 1838\sphinxhyphen{}1865 (2013).}

\end{itemize}

\sphinxstepscope


\chapter{Stochastic processes}
\label{\detokenize{ch/prob/processes:stochastic-processes}}\label{\detokenize{ch/prob/processes:prob-processes}}\label{\detokenize{ch/prob/processes::doc}}
\sphinxAtStartPar
\sphinxstylestrong{Definition of stochastic process.}

\sphinxAtStartPar
\sphinxstylestrong{Examples.}
\begin{itemize}
\item {} 
\sphinxAtStartPar
White noise, \(\xi(t)\), is a zero\sphinxhyphen{}mean process with no correlation between its values at different times
\begin{equation*}
\begin{split}\mathbb{E}\left[ \xi(t) \, \xi(s) \right] = \delta(t-s)\end{split}
\end{equation*}
\item {} 
\sphinxAtStartPar
Wiener process (Brownian motion), \(W(t)\)
\begin{equation*}
\begin{split}\begin{aligned}
    & W(0) = 0 \\
    & W(t) \text{ has independent increments} \\
    & W(t) - W(s) \sim N(0, t-s) \text{ for $t > s$} \\
    & W(t) \text{ are continuous but nowhere differentiable}
  \end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
Informal relation between Wiener process and white noise signal
\begin{equation*}
\begin{split}\begin{aligned}
   W(t) - W(s) & = \int_{s}^{t} \xi(\tau) \, d \tau \\
   {}^{''} \dfrac{d W(t)}{d t} & = \xi(t) {}^{``}
  \end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
where the derivative relation doesn’t hold in the classical sense, as \(W(t)\) is nowhere differentiable

\item {} 
\sphinxAtStartPar
time\sphinxhyphen{}discrete Markov processes

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Applications}
\begin{itemize}
\item {} 
\sphinxAtStartPar
LTI

\item {} 
\sphinxAtStartPar
Stochastic differential equations…
\begin{equation*}
\begin{split}d X(t) = \mu(t) \, dt + \sigma(t) \, dW(t)\end{split}
\end{equation*}
\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Assumptions.}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Stationariety

\item {} 
\sphinxAtStartPar
Ergodicity

\end{itemize}
\begin{equation*}
\begin{split}k_{xy}(\tau) := \mathbb{E}[x(t) y(t-\tau)] = \lim_{T\rightarrow +\infty} \left\{ \frac{1}{2T} \int_{t = -T}^{T} x(t) \, y(t-\tau) \, d t \right\}\end{split}
\end{equation*}
\sphinxstepscope


\section{Wiener process \sphinxhyphen{} Brownian motion}
\label{\detokenize{ch/prob/wiener:wiener-process-brownian-motion}}\label{\detokenize{ch/prob/wiener:prob-processes-wiener}}\label{\detokenize{ch/prob/wiener::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Introduction: history and relation with other problems (diffusion?)

\item {} 
\sphinxAtStartPar
Definition and some theory

\item {} 
\sphinxAtStartPar
Simulation of Wiener process, demonstration of properties shown in theory section

\end{itemize}


\subsection{Definition}
\label{\detokenize{ch/prob/wiener:definition}}\label{\detokenize{ch/prob/wiener:prob-processes-wiener-def}}\label{ch/prob/wiener:wiener:def}
\begin{sphinxadmonition}{note}{Definition 4.1.1 (Wiener process \sphinxhyphen{} Brownian motion)}



\sphinxAtStartPar
A Wiener process is a random process \(W(t)\) with
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
initial condition, almost surely

\end{enumerate}
\begin{equation*}
\begin{split}W(0) = 0\end{split}
\end{equation*}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
increments with zero\sphinxhyphen{}mean {\hyperref[\detokenize{ch/prob/rv-continuous:prob-rv-continuous-ex-normal}]{\sphinxcrossref{\DUrole{std,std-ref}{normal distribution}}}}

\end{enumerate}
\begin{equation*}
\begin{split}W(t) - W(s) \sim \mathscr{N}(0,|t-s|)\end{split}
\end{equation*}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
\(W\) has independent increments: \(W(t) - W(t+u)\) is independent from \(W_s\), \(s < t\)

\item {} 
\sphinxAtStartPar
in \(W(t)\) is almost surely continuous in \(t\)

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Almost sure convergence in statistics}

\sphinxAtStartPar
“Almost surely” here means {\hyperref[\detokenize{ch/prob/convergence:prob-convergence-strong}]{\sphinxcrossref{\DUrole{std,std-ref}{almost sure converngece}}}} and it is explained in the section dealing with {\hyperref[\detokenize{ch/prob/convergence:prob-convergence}]{\sphinxcrossref{\DUrole{std,std-ref}{convergence in statistics}}}}, and used below to prove some properties of a Wiener process.
\end{sphinxadmonition}




\subsection{Properties}
\label{\detokenize{ch/prob/wiener:properties}}\label{ch/prob/wiener:property-1}
\begin{sphinxadmonition}{note}{Property 4.1.1 (Covariance of increments)}



\sphinxAtStartPar
Covariance of an increment follows the definition of Wiener process and the definition of {\hyperref[\detokenize{ch/prob/rv-continuous:prob-rv-continuous-ex-normal}]{\sphinxcrossref{\DUrole{std,std-ref}{normal distribution}}}},
\begin{equation}\label{equation:ch/prob/wiener:eq:wiener:cov:1}
\begin{split}\mathbb{E}\left[ \left( W(t) - W(s) \right)^2  \right] = \mathbb{E}\left[ \mathscr{N}(0, |t-s|) \right] =  |t - s| \ .\end{split}
\end{equation}
\sphinxAtStartPar
Covariance of independent increments \sphinxhyphen{} on non\sphinxhyphen{}overlapping ranges \sphinxhyphen{} is zero, as {\hyperref[\detokenize{ch/prob/rv-multi-dimensional:prob-multidim-independence-no-correlation}]{\sphinxcrossref{\DUrole{std,std-ref}{independence implies no correlation}}}}, i.e. zero covariance. Thus, if \(a \le b \le c \le d\), \(W(b)-W(a)\) and \(W(d)-W(c)\) are independent by property \((3)\) in {\hyperref[\detokenize{ch/prob/wiener:wiener:def}]{\sphinxcrossref{Definition 4.1.1}}} of Wiener process, and thus their covariance \sphinxhyphen{} and correlation \sphinxhyphen{} is zero,
\begin{equation}\label{equation:ch/prob/wiener:eq:wiener:cov:2}
\begin{split}\mathbb{E}\left[ ( W(b) - W(a) ) ( W(d) - W(c) )  \right] = 0\end{split}
\end{equation}
\sphinxAtStartPar
Covariance of two generic increments reads
\begin{equation}\label{equation:ch/prob/wiener:eq:wiener:cov:3}
\begin{split}\mathbb{E}\left[ \left( W(t_1) - W(s_1) \right)  \left( W(t_2) - W(s_2) \right) \right] = \big| [s_1, t_1] \cap [s_2, t_2]\big|\end{split}
\end{equation}
\sphinxAtStartPar
as it’s proved below.
\end{sphinxadmonition}
\subsubsection*{Proof of the covariance of two generic increments }

\sphinxAtStartPar
If \(s_1 \le s_2 \le t_2 \le t_1\),
\begin{equation*}
\begin{split}\begin{aligned}
  & \mathbb{E}\left[ \left( W(t_1) - W(s_1) \right)  \left( W(t_2) - W(s_2) \right) \right] = \\
  & \quad = \mathbb{E}\left[ \left( W(t_1) - W(t_2) + W(t_2) - W(s_2) + W(s_2) - W(s_1) \right) \left( W(t_2) - W(s_2) \right) \right] = \\
  & \quad = \underbrace{\mathbb{E}\left[ \left( W(t_1) - W(t_2) \right) (W(t_2) - W(s_2) ) \right]}_{=0} 
          + \underbrace{\mathbb{E}\left[ \left( W(t_2) - W(s_2) \right) (W(t_2) - W(s_2) ) \right]}_{=|t_2 - s_2|} + \\
  & \quad + \underbrace{\mathbb{E}\left[ \left( W(s_2) - W(s_1) \right) (W(t_2) - W(s_2) ) \right]}_{= 0} = \\
  & \quad  = 0 + |t_2 - s_2| + 0 = \big| [s_1, t_1] \cap [s_2, t_2] \big| \ . 
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
Similarly, if \(s_1 \le s_2 \le t_1 \le t_2 \),
\begin{equation*}
\begin{split}\begin{aligned}
  & \mathbb{E}\left[ \left( W(t_1) - W(s_1) \right)  \left( W(t_2) - W(s_2) \right) \right] = \\
  & \quad = \mathbb{E}\left[ \left( W(t_1) - W(s_2) + W(s_2) - W(s_1) \right) \left( W(t_2) - W(t_1) + W(t_1) - W(s_2) \right) \right] = \\
  & \quad = \underbrace{\mathbb{E}\left[ \left( W(t_1) - W(s_2) \right) (W(t_2) - W(t_1) ) \right]}_{=0} 
          + \underbrace{\mathbb{E}\left[ \left( W(t_1) - W(s_2) \right) (W(t_1) - W(s_2) ) \right]}_{=|t_1 - s_2|} + \\
  & \quad + \underbrace{\mathbb{E}\left[ \left( W(s_2) - W(s_1) \right) (W(t_2) - W(t_1) ) \right]}_{= 0}  
          + \underbrace{\mathbb{E}\left[ \left( W(s_2) - W(s_1) \right) (W(t_1) - W(s_2) ) \right]}_{= 0} = \\
  & \quad  = 0 + |t_1 - s_2| + 0 = \big| [s_1, t_1] \cap [s_2, t_2] \big| \ . 
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
All the other situations can be proved in the same way.
\label{ch/prob/wiener:property-2}
\begin{sphinxadmonition}{note}{Property 4.1.2 (Statistics of maximum)}



\sphinxAtStartPar
For \(a \ge 0\),
\begin{equation*}
\begin{split}P\left( M(t) \ge a  \right) = 2 P \left( W(t) \ge a \right) = 2 - 2 \, \phi\left( \frac{a}{\sqrt{t}} \right) \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
with
\begin{equation*}
\begin{split}M(t) = \max_{0 \le \tau \le t} W(\tau)\end{split}
\end{equation*}
\sphinxAtStartPar
and
\begin{equation*}
\begin{split}\phi(x) = \int_{y = -\infty}^{x} p_{\mathscr{N}(0,1)}(x) \, dx\end{split}
\end{equation*}
\sphinxAtStartPar
is the cumulative probability function of a normal distribution \(\mathscr{N}(0,1)\).
\end{sphinxadmonition}
\subsubsection*{Proof.}

\sphinxAtStartPar
\sphinxstylestrong{The second inequality} immediately follows from the very definition of Wiener process with initial conditions \(W(0)\),
\begin{equation*}
\begin{split}\begin{aligned}
  P \left( W(t) - W(0) \ge a \right) 
  & = P \left( \mathscr{N}(0,t) \ge a \right) = && (1) \\
  & = P \left( \mathscr{N}\left(0, 1 \right) > \frac{a}{\sqrt{t}} \right) = \\
  & = \int_{x=\frac{a}{\sqrt{t}}}^{+\infty} p(y) \, dy = && (2) \\
  & = 1 - \int_{x=-\infty}^{\frac{a}{\sqrt{t}}} p(y) \, dy = 1 - \phi \left( \frac{a}{\sqrt{t}}  \right)
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
having used \((1)\) scaling rule for {\hyperref[\detokenize{ch/prob/transformations:prob-transformations}]{\sphinxcrossref{\DUrole{std,std-ref}{transformation of probability functions}}}}… \sphinxstylestrong{todo}, and \((2)\) the normalization condition of the probability density \(1 = \int_{x = -\infty}^{+\infty} p(x) \, dx\), and the definition of cumulative probability function.

\sphinxAtStartPar
\sphinxstylestrong{First inequality.} In order to prove the first inequality, it could be useful to introduce the definition of \sphinxstylestrong{stepping time}, \(\tau_a\), as the random variable defined as
\begin{equation*}
\begin{split}\tau_a = \min_s \left\{ s: W(s) = a \right\} \ .\end{split}
\end{equation*}
\sphinxAtStartPar
Using \sphinxstyleemphasis{reflection principle}, it follows
\begin{equation*}
\begin{split}\begin{aligned}
 P( M(t) \ge a ) 
 & = && (1) \\
 & = P( M(t) \ge a, W(t) \ge a) + P( M(t) \ge a,  W(t) < a) = && (2) \\
 & = P( W(t) \ge a ) + P( M(t) \ge a,  W(t) - W(\tau_a) < 0) = && (3) \\
 & = P( W(t) \ge a ) + P( M(t) \ge a,  W'(t - \tau_a) < 0) = && (4) \\
 & = P( W(t) \ge a ) + P( M(t) \ge a ) P (  W'(t - \tau_a) < 0) = && (5) \\
 & = P( W(t) \ge a ) + \dfrac{1}{2} P( M(t) \ge a ) \ .
 \end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
haing \((1)\) used “marginalization” to write \(P(A) = P(A,B) + P(A, \overline{B})\), \((2)\) recognized that if \(B: \, W(t) \ge a\) then \(A: \, M(t) \ge a\) or \(B \subseteq A\), and thus \(P(A,B) = P(B)\), and that \(a = W(\tau_a)\), \((3)\) defined the Wiener process \(W'(t - \tau_a) := W(t) - W(\tau_a)\), independent from \(W(s)\), \(0 \le s \le \tau_a\), \((4)\) exploited the independence of the two conditions (\sphinxstylestrong{todo} \sphinxstyleemphasis{be more explicit, proof needed?}), \((5)\) and the symmetry of Wiener process to get \(P(W'(t-\tau_a) < 0) = \frac{1}{2}\).

\sphinxAtStartPar
Thus, it follows the requied relation
\begin{equation*}
\begin{split}P(M(t) \ge a) = 2 P(W(t) \ge a) \ .\end{split}
\end{equation*}

\label{ch/prob/wiener:property-3}
\begin{sphinxadmonition}{note}{Property 4.1.3 (\protect\(W(t)\protect\) is almost surely not differentiable)}



\sphinxAtStartPar
For all time \(t\), a Wiener process is almost surely not differentiable, i.e. …\sphinxstylestrong{todo}
\end{sphinxadmonition}
\subsubsection*{Proof.}

\sphinxAtStartPar
\sphinxstylestrong{todo} \sphinxstyleemphasis{check details}

\sphinxAtStartPar
Wiener process is differentiable in \(t\) if the limit
\begin{equation*}
\begin{split}\lim_{h \rightarrow 0} \dfrac{W(t+h) - W(t)}{h} = \ell\end{split}
\end{equation*}
\sphinxAtStartPar
exists finite. Definition of limit reads,
\begin{equation*}
\begin{split}\forall \varepsilon > 0 \quad \exists U_{0,\delta} \quad \text{s.t.} \quad \left| \, \frac{W(t+h) - W(t)}{h} - \ell \, \right| < \varepsilon \qquad \forall h \in U_{0,\delta} \backslash \{ 0 \}\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{todo} \sphinxstyleemphasis{how to go from this definition to the following one?}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Let \(E_{\varepsilon, A, t_0}\) be the event s.t. for a given \(t_0\), \(W(t)\) is differentiable in \(t_0\), i.e. \(\exists\) \(A\), \(\varepsilon_0\) const.  s.t. \(\forall \varepsilon\) s.t. \(0 < \varepsilon < \varepsilon_0\), \(W(t) - W(t_0) \le A \varepsilon\) holds for \(\forall \varepsilon\), \(0 < t - t_0 \le \varepsilon\).

\sphinxAtStartPar
Let \(E_{A, t_0} = \cap_{\varepsilon} E_{\varepsilon, A, t_0}\). Then
\begin{equation*}
\begin{split}\begin{aligned}
  P\left( E_{\varepsilon, A, t_0} \right) 
  & = P \left( |W(t) - W(t_0)| \le A \varepsilon \text{ for } \forall t-t_0 \text{ s.t. } 0 < t - t_0 \le \varepsilon \right) =  && (1) \\
  & = P \left( M(t - t_0)| \le A \varepsilon \right) = && (2) \\
  & = 1 - P \left( M(t - t_0)| \ge A \varepsilon \right) = && (3) \\
  & = 1 - \left[ 2 - 2 \phi \left( \dfrac{A \varepsilon}{\sqrt{\Delta t}} \right) \right] = \\
  & = - 1 + 2 \phi \left( \dfrac{A \varepsilon}{\sqrt{\Delta t}} \right)  \ ,
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
having used \((1)\)…, \((2)\)…, \((3)\)…

\sphinxAtStartPar
Now, being \(\varepsilon \le \Delta t\), it follows that \(\frac{\varepsilon}{\sqrt{\Delta t}} \le \sqrt{\Delta t}\). As \(\varepsilon \rightarrow 0\), then \(\frac{\varepsilon}{\sqrt{\Delta t}} \rightarrow 0\), and \(\phi\left( \frac{A \varepsilon}{\sqrt{\Delta t}} \right) \rightarrow \frac{1}{2}\), and \(P(E_{\varepsilon,A,t_0}) \rightarrow 0\)

\sphinxstepscope


\section{White noise}
\label{\detokenize{ch/prob/white-noise:white-noise}}\label{\detokenize{ch/prob/white-noise:prob-processes-wn}}\label{\detokenize{ch/prob/white-noise::doc}}\label{ch/prob/white-noise:wn:def:properties}
\begin{sphinxadmonition}{note}{Definition 4.2.1 (White noise \sphinxhyphen{} properties)}



\sphinxAtStartPar
A white noise is a random process with
\begin{itemize}
\item {} 
\sphinxAtStartPar
zero expected value
\begin{equation*}
\begin{split}\mathbb{E}[ \xi(t) ] = 0\end{split}
\end{equation*}
\item {} 
\sphinxAtStartPar
Dirac delta correlation
\begin{equation*}
\begin{split}\mathbb{E}[ \xi(t) \xi(s) ] = \delta(t-s)\end{split}
\end{equation*}
\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
\sphinxstylestrong{todo} \sphinxstyleemphasis{link to math:functional\sphinxhyphen{}analysis:distributions}
\label{ch/prob/white-noise:wn:def:derivative}
\begin{sphinxadmonition}{note}{Definition 4.2.2 (White noise \sphinxhyphen{} time derivative of Wiener process \protect\(W(t)\protect\) in the sense of distributions)}


\end{sphinxadmonition}

\sphinxstepscope


\section{Stochastic calculus}
\label{\detokenize{ch/prob/processes-calculus:stochastic-calculus}}\label{\detokenize{ch/prob/processes-calculus:prob-processes-calculus}}\label{\detokenize{ch/prob/processes-calculus::doc}}

\subsection{Ito’s lemma}
\label{\detokenize{ch/prob/processes-calculus:ito-s-lemma}}\label{\detokenize{ch/prob/processes-calculus:prob-processes-calculus-ito-lemma}}
\sphinxAtStartPar
It allows to find the differential of a time\sphinxhyphen{}dependent function of a stochastic process. Let \(f(t,x)\) be a twice\sphinxhyphen{}differentiable scalar function. Its Taylor series gives
\begin{equation*}
\begin{split}
\Delta f = \dfrac{\partial f}{\partial t} \Delta t + \dfrac{\partial f}{\partial x} \Delta x 
  + \dfrac{1}{2}\dfrac{\partial^2 f}{\partial t^2} \Delta t^2  
  +             \dfrac{\partial^2 f}{\partial t \partial x} \Delta t \, \Delta x  
  + \dfrac{1}{2}\dfrac{\partial^2 f}{\partial x^2} \Delta x^2 
\end{split}
\end{equation*}
\sphinxAtStartPar
If the argument \(x\) of the function \(f\) is chosen to be a random process \(X_t\) satisfying {\hyperref[\detokenize{ch/prob/processes-calculus:prob-processes-calculus-ito-process-drift-diffusion}]{\sphinxcrossref{\DUrole{std,std-ref}{Ito drift\sphinxhyphen{}diffusion process}}}},
\begin{equation*}
\begin{split}d X_t = \mu_t \, dt + \sigma_t \, dW_t \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
the differential of function \(f(t,X_t)\) results from the limit of Taylor series
\begin{equation*}
\begin{split}\begin{aligned}
  df
  & = \lim_{dt \rightarrow 0, dW_t \rightarrow 0} \left\{ \Delta f \right\} = \\
  & = \lim_{dt \rightarrow 0, dW_t \rightarrow 0} \left\{ \partial_t f dt + \partial_x f d X_t + \dfrac{1}{2} \left[ \partial_{tt} f \, dt^2 + 2 \partial_{xt} \, dt dX_t + \partial_{xx} f dX_t^2 \right] \right\} = \\
  & = \lim_{dt \rightarrow 0, dW_t \rightarrow 0} \left\{ \partial_t f dt + \partial_x f \left( \mu_t dt + \sigma_t dW_t \right) + \dfrac{1}{2} \left[ \partial_{tt} f \, dt^2 + 2 \partial_{xt} \, dt \left( \mu_t dt + \sigma_t dW_t \right) + \partial_{xx} f \left( \mu_t dt + \sigma_t dW_t \right)^2 \right] \right\} = \\
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
For \(dt \rightarrow 0\), \(\left(d W_t\right)^2 = O(dt)\); keeping only terms of order lower than or equal to \(O(dt)\), the differential becomes,
\begin{equation*}
\begin{split}df = \left( \partial_t f + \mu_t \partial_x f  \right) dt + \sigma_t \partial_x f \, d W_t + \dfrac{\sigma_t^2}{2} \partial_{xx} f \, dW_t^2 \ .\end{split}
\end{equation*}
\sphinxAtStartPar
Replacing \(dW_t^2\) with \(d t\) \sphinxstylestrong{todo} \sphinxstyleemphasis{why?}, and recalling the SDE of the Ito drift\sphinxhyphen{}diffusion process,
\begin{equation*}
\begin{split}\begin{aligned}
  df
  & = \left( \partial_t f + \mu_t \partial_x f + \dfrac{\sigma_t^2}{2} \partial_{xx} f \right) dt + \sigma_t \partial_x f \, d W_t = \\
  & = \left( \partial_t f +  \dfrac{\sigma_t^2}{2} \partial_{xx} f \right) dt + \partial_x f \left( \mu \, dt + \sigma_t \, d W_t \right) = \\
  & = \left( \partial_t f +  \dfrac{\sigma_t^2}{2} \partial_{xx} f \right) dt + \partial_x f \, dX_t \ .
\end{aligned}\end{split}
\end{equation*}

\subsection{Ito’s calculus}
\label{\detokenize{ch/prob/processes-calculus:ito-s-calculus}}\label{\detokenize{ch/prob/processes-calculus:prob-processes-calculus-ito-calculus}}
\sphinxAtStartPar
Integegration w.r.t. Browinan motion produces a random variable that can be defined as
\begin{equation*}
\begin{split}\int_{0}^{t} F \, dW := \lim_{n \rightarrow +\infty} \sum_{[t_{i-1},t_i] \in \pi_n} F_{t_{i-1}} \left( W_{t_i} - W_{t_{i-1}} \right) \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
being \(\pi_n\) a partition of interval \([0,t]\), and \(H\) a random proces \sphinxstylestrong{todo} \sphinxstyleemphasis{with some characteristics…}
\label{ch/prob/processes-calculus:example-0}
\begin{sphinxadmonition}{note}{Example 4.3.1 (Integral of a Brownian motion w.r.t. itself)}


\begin{equation*}
\begin{split}Y(t) = \int_{s=0}^t W_s \, dW_s = \dfrac{1}{2} W_t^2 - \dfrac{t}{2} \ .\end{split}
\end{equation*}
\sphinxAtStartPar
The expected value for each \(t\) of the random process \(Y_t\) is zero for all \(t\), \(\mathbb{E}\left[ W_t^2 \right]= 0\), as the expected value of \(W_t^2\) is the variance of \(W_t\), and thus \(t\) by definition of the Wiener process.
\subsubsection*{Evaluation of the integral}

\sphinxAtStartPar
Let \(f(t,x) = x^2\). Let’s find the differential \(df\) evaluated for \(x = W_t\) using {\hyperref[\detokenize{ch/prob/processes-calculus:prob-processes-calculus-ito-lemma}]{\sphinxcrossref{\DUrole{std,std-ref}{Ito’s lemma}}}}, retaining only terms with order up to \(O(dt)\). Since \(\partial_t f \equiv 0\),
\begin{equation*}
\begin{split}\begin{aligned}
  df & = \partial_x f|_{x=W_t} dW_t + \dfrac{1}{2} \partial_{xx} f|_{x=W_t} dW_t^2 
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
and thus, replacing \(dW_t^2 = dt\),
\begin{equation*}
\begin{split}\begin{aligned}
  d W_t^2 & =  2 W_t \, dW_t + dt \ .
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
or
\begin{equation*}
\begin{split}\begin{aligned}
  W_t \, d W_t & = d \left( \dfrac{W_t^2}{2} \right) - \dfrac{dt}{2} \ .
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
Thus (\sphinxstylestrong{todo} \sphinxstyleemphasis{add details if needed. A bit too much freedom in using differentials over stochastic processes here}),
\begin{equation*}
\begin{split}\begin{aligned}
  Y(t)
  & = \int_{s=0}^t W_s \, dW_s \, ds = \\
  & = \int_{s=0}^t \left( \dfrac{W_s}{2} \right) \, ds - \int_{s=0}^t \dfrac{1}{2} \, ds = \\
  & = \dfrac{1}{2} \left( W_t^2 - W_0^2 \right) - \dfrac{t}{2} \ .
\end{aligned}\end{split}
\end{equation*}\end{sphinxadmonition}


\subsection{Ito processes}
\label{\detokenize{ch/prob/processes-calculus:ito-processes}}\label{\detokenize{ch/prob/processes-calculus:prob-processes-calculus-ito-process}}

\subsubsection{Ito drift\sphinxhyphen{}diffusion process}
\label{\detokenize{ch/prob/processes-calculus:ito-drift-diffusion-process}}\label{\detokenize{ch/prob/processes-calculus:prob-processes-calculus-ito-process-drift-diffusion}}
\sphinxAtStartPar
An Ito drift\sphinxhyphen{}diffusion process is a stochastic process satisfying the stochastic differential equation (SDE)
\begin{equation}\label{equation:ch/prob/processes-calculus:eq:ito:drift-diff}
\begin{split}dX_t = \mu_t \, dt + \sigma_t \, dW_t \ ,\end{split}
\end{equation}
\sphinxAtStartPar
with \(W_t\) a Wiener process. If \(\mu_t = \mu\), \(\sigma_t = \sigma\) are constant a closed\sphinxhyphen{}form solution can be found using {\hyperref[\detokenize{ch/prob/processes-calculus:prob-processes-calculus-ito-lemma}]{\sphinxcrossref{\DUrole{std,std-ref}{Ito’s lemma}}}}, for \(f(t,x) = x\), or by direct (stochastic) integration of the SDE \eqref{equation:ch/prob/processes-calculus:eq:ito:drift-diff}, as
\begin{equation*}
\begin{split}\begin{aligned}
  \int_{s=0}^t dX_s & = \int_{s=0}^t \mu \, ds + \int_{s=0}^t \sigma \, dW_s \\
\end{aligned}\end{split}
\end{equation*}\begin{equation*}
\begin{split}\begin{aligned}
  X_t - X_0 & = \mu t + \sigma \left( W_t - W_0 \right) \ ,
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
so that \(X_t - X_0 \sim \mathscr{N}\left( \mu t, \sigma^2 t \right)\).

\begin{sphinxadmonition}{note}{Scaling of a Wiener process}

\sphinxAtStartPar
Term \(\sigma W_t\) represents a scaling of a Wiener process \(W_t \sim \mathscr{N}(0, t)\) with zero expected value and variance \(t\). Multiplication by factor \(\sigma\) results in a multiplication of the expected value by \(\sigma\) and variance by \(\sigma^2\).
\end{sphinxadmonition}


\subsubsection{Geometric Brownian Motion, GBM}
\label{\detokenize{ch/prob/processes-calculus:geometric-brownian-motion-gbm}}\label{\detokenize{ch/prob/processes-calculus:prob-processes-calculus-ito-process-gbm}}
\sphinxAtStartPar
A geometric Brownian motion is a stochastic process satisfying the SDE
\begin{equation*}
\begin{split}d X_t = \mu X_t \, dt + \sigma X_t \, dW_t \ .\end{split}
\end{equation*}\label{ch/prob/processes-calculus:example:gbm:finance}
\begin{sphinxadmonition}{note}{Example 4.3.2 (GBM in Finance)}



\sphinxAtStartPar
GBM can be used as a model of the price of an asset with constant expected return and variance of returns with normal distribution.
\end{sphinxadmonition}

\sphinxAtStartPar
Let \(f(x) = \ln x\) be evaluated for \(x = X_t\). Ito’s lemma, with \(\partial_t f \equiv 0\), provides the expression of the differential
\begin{equation*}
\begin{split}\begin{aligned}
  d f
  & = \partial_x f|_{X_t} d X_t + \dfrac{1}{2} \partial_{xx} f|_{X_t} d X_t^2 = \\
  & = \partial_x f|_{X_t} \left( \mu X_t \, dt + \sigma X_t \, dW_t \right) + \dfrac{1}{2} \partial_{xx} f|_{X_t}  \left( \mu X_t \, dt + \sigma X_t \, dW_t \right)^2 = \\
  & = \dfrac{1}{X_t} \left( \mu X_t \, dt + \sigma X_t \, dW_t \right) - \dfrac{1}{2} \dfrac{1}{X_t^2} \sigma^2 X^2_t \, dW^2_t = \\
  d \left( \ln X_t \right) & = \left( \mu - \dfrac{\sigma^2}{2} \right) \, dt + \sigma \, dW_t \ ,
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
whose solution after integration reads
\begin{equation*}
\begin{split}\ln X_t = \ln X_0 + \left( \mu - \dfrac{\sigma^2}{2} \right) \, t + \sigma \, W_t \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
or
\begin{equation*}
\begin{split}X_t = X_0 \, e^{\left( \mu - \frac{\sigma^2}{2} \right) \, t + \sigma W_t} \ .\end{split}
\end{equation*}

\subsubsection{Geometric Brownian Motion with drift}
\label{\detokenize{ch/prob/processes-calculus:geometric-brownian-motion-with-drift}}\label{\detokenize{ch/prob/processes-calculus:prob-processes-calculus-ito-process-gbm-drift}}
\sphinxAtStartPar
A geometric Brownian motion is a stochastic process satisfying the SDE
\begin{equation*}
\begin{split}d X_t = \mu X_t \, dt - C \, dt + \sigma X_t \, dW_t \ .\end{split}
\end{equation*}\label{ch/prob/processes-calculus:example:gbm-drift:finance}
\begin{sphinxadmonition}{note}{Example 4.3.3 (GBM with constant withdrawal in finance)}



\sphinxAtStartPar
GBM with drift can be used in finance as a model to represent DCA strategy and pension withdrawal, and to show and discuss \sphinxstylestrong{sequence risk}.
\end{sphinxadmonition}

\sphinxAtStartPar
The solution reads
\begin{equation*}
\begin{split}X_t = X_0 e^{\left( \mu - \frac{\sigma^2}{2} \right) (t-t_0) + \sigma ( W_t - W_0)} + \int_{s=0}^{t} C e^{\left( \mu - \frac{\sigma^2}{2} \right) (t-s) + \sigma ( W_t - W_s)} \, ds \ .\end{split}
\end{equation*}\subsubsection*{Integration factor method for linear SDEs}

\sphinxAtStartPar
Integration factor method for linear SDEs
\begin{equation*}
\begin{split}d X_t = a \, dt + b \, dW_t \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
with \(a(X_t, t, W_t)\), \(b(X_t, t, W_t)\)

\sphinxAtStartPar
aims at finding an exponential factor \(e^{\alpha t + \beta W_t}\) that allows to get an integrable expression of the differential
\begin{equation*}
\begin{split}d \left( e^{\alpha t + \beta W_t} X_t \right) = d \, f\left( t, W_t, X_t \right) \ .\end{split}
\end{equation*}
\sphinxAtStartPar
Taylor expansion of this expression up to terms of order \(dt \sim dW_t^2\) reads
\begin{equation*}
\begin{split}\begin{aligned}
  d \, f(t,W_t,X_t)
  & = \partial_t f \, dt + \partial_w f \, d W_t + \partial_x f \, \underbrace{dX_t}_{a dt + b dW_t} + \\
  & + \dfrac{1}{2} \left( \underbrace{\partial_{tt} f \, dt^2}_{o(dt)} + \partial_{ww} f \, \underbrace{dW_t^2}_{dt} + \partial_{xx}  f\, \underbrace{dX_t^2}_{b^2 \, dW_t^2 = b^2 \, dt} + \underbrace{2 \partial_{tw} f \, dt \, dW_t + 2 \partial_{tx} f \, dt \, dX_t}_{o(dt)} + 2 \partial_{xw} f \, \underbrace{dW_t \, dX_t}_{b dW_t^2 = b dt}  \right) = \\
  & = dt \left[ \partial_t f + a \partial_x f + \dfrac{1}{2} \partial_{ww} f + \dfrac{1}{2} b^2 \partial_{xx} f + 2 b \partial_{xw} f \right] + d W_t \left[ \partial_w f + b \partial_x f \right] \\
\end{aligned}\end{split}
\end{equation*}\subsubsection*{Proof (with integration factor method, for linear SDEs)}

\sphinxAtStartPar
GBM motion with drift and constant coefficients is governed by SDE
\begin{equation*}
\begin{split}d X_t = \mu X_t \, dt - C \, dt + \sigma X_t \, dW_t \ .\end{split}
\end{equation*}
\sphinxAtStartPar
Referring to the general expression of SDEs, coefficients \(a\), \(b\) of the GBM with drift read
\begin{equation*}
\begin{split}\begin{aligned}
  a & = \mu \, X_t + C \\
  b & = \sigma \, X_t \ .
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
Partial derivatives of function \(f = e^{\alpha t + \beta w} \, x\) appearing in the solution of SDEs through integration factor method read
\begin{equation*}
\begin{split}\begin{aligned}
  \partial_t    f & = e^{\alpha t + \beta w} \, x \, \alpha   \\
  \partial_w    f & = e^{\alpha t + \beta w} \, x \, \beta    \\
  \partial_x    f & = e^{\alpha t + \beta w}                  \\
  \partial_{xx} f & = 0                                       \\
  \partial_{wx} f & = e^{\alpha t + \beta w}      \, \beta    \\
  \partial_{ww} f & = e^{\alpha t + \beta w} \, x \, \beta^2  \\
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
It’s now possible to simplify the RHS of the the expression of the differential \(df\). Namely, it’s possible to choose values of \(\alpha\), \(\beta\) in order to get simpler expressions of the factors of the differentials \(dt\)  and \(d W_t\)
\begin{equation*}
\begin{split}\begin{aligned}
  dW_t: \quad  \partial_{W_t} f 
  & = \left.\left( \partial_w f + b \partial_x f \right)\right|_{t,W_t,X_t} = \\
  & = e^{\alpha t + \beta W_t} \, \left( X_t \beta + b \right) = \\
  & = e^{\alpha t + \beta W_t} \, X_t \left( \beta + \sigma \right) \\
  dt  : \quad  \partial_{  t} f  \quad 
  & = \left.\left[ \partial_t f + a \partial_x f + \dfrac{1}{2} \partial_{ww} f + \dfrac{1}{2} b^2 \partial_{xx} f + b \partial_{xw} f \right]\right|_{t,W_t,X_t} \\
  & = e^{\alpha t + \beta W_t} \left[ X_t \alpha + a + \dfrac{1}{2} X_t \beta^2 + 0 + b \beta \right] = \\
  & = e^{\alpha t + \beta W_t} \left[ X_t \alpha + \mu X_t + C + \dfrac{1}{2} X_t \beta^2 + \sigma X_t \beta \right] = \\
  & = e^{\alpha t + \beta W_t} \left[ X_t \left( \alpha + \mu + \dfrac{1}{2} \beta^2 + \sigma \beta \right) + C \right] \ .
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
Setting
\begin{equation*}
\begin{split}\begin{aligned}
  \beta  & = - \sigma \\ 
  \alpha & = - \mu - \dfrac{1}{2} \beta^2 - \sigma \beta = - \mu + \dfrac{\sigma^2}{2} \ , 
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
the differential \(d f\) becomes
\begin{equation*}
\begin{split}d\left( e^{\left( -\mu + \frac{\sigma^2}{2} \right) t - \sigma W_t} \, X_t \right) = C e^{\left( -\mu + \frac{\sigma^2}{2} \right) t - \sigma W_t}\end{split}
\end{equation*}
\sphinxAtStartPar
and integration gives
\begin{equation*}
\begin{split}X_t = X_0 e^{\left( \mu - \frac{\sigma^2}{2} \right) (t-t_0) + \sigma ( W_t - W_0)} + \int_{s=0}^{t} C e^{\left( \mu - \frac{\sigma^2}{2} \right) (t-s) + \sigma ( W_t - W_s)} \, ds \ .\end{split}
\end{equation*}
\sphinxstepscope


\part{Statistical Inference}

\sphinxstepscope


\chapter{Introduction to Statistical Inference}
\label{\detokenize{ch/infer/intro:introduction-to-statistical-inference}}\label{\detokenize{ch/infer/intro:infer-intro}}\label{\detokenize{ch/infer/intro::doc}}
\sphinxstepscope


\part{Introduction to Machine Learning}

\sphinxstepscope


\chapter{Introduction to Machine Learning}
\label{\detokenize{ch/ml/intro:introduction-to-machine-learning}}\label{\detokenize{ch/ml/intro:ml-intro}}\label{\detokenize{ch/ml/intro::doc}}
\sphinxAtStartPar
Artificial intelligence can be broadly defined as a field dealing with making machines perform tasks that require intelligence, when performed by humans, like: reasoning, perception, representation, language processing, planning, learning

\sphinxAtStartPar
\sphinxstylestrong{Machine learning} is a branch of AI focused on statistical algoritms that can \sphinxstylestrong{learn from data} and \sphinxstylestrong{generalize to unseen data} and perform tasks, without explicit instructions.%
\begin{footnote}[1]\sphinxAtStartFootnote
“Without explicit instructions” means that a systems has no user\sphinxhyphen{}coded behavior, but learns it usually via \sphinxstylestrong{optimization}, usually either involving minimization of an error function or maximization of an objective function or energy/information content.
%
\end{footnote}

\sphinxAtStartPar
\sphinxstylestrong{Three core paradigms.} Algorithms in machine learning can be divided into three paradigms:
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{ch/ml/sl:ml-sl}]{\sphinxcrossref{\DUrole{std,std-ref}{\sphinxstylestrong{Supervised Learning, SL}}}}}: algorithm learns from labelled data; many applications can be reduced to 2 main tasks: \sphinxstylestrong{regression} (or function approximation) and \sphinxstylestrong{classification}.

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{ch/ml/ul:ml-ul}]{\sphinxcrossref{\DUrole{std,std-ref}{\sphinxstylestrong{Unsupervised Learning, UL}}}}}: algorithm learns pattern from un\sphinxhyphen{}labelled data; examples of taks in UL are clustering, dimensionality reduction (and recognition of \sphinxstyleemphasis{main} components in data), compression (retaining only relevant components in data). Some historical algorithms and linear algebra decompositions can be interpreted or generalized as unsupervised learning.

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{ch/ml/rl:ml-rl}]{\sphinxcrossref{\DUrole{std,std-ref}{\sphinxstylestrong{Reinforcement Learning, RL}}}}}: an algorithm (\sphinxstylestrong{agent}) learns a \sphinxstylestrong{policy} \sphinxhyphen{} i.e. the way to behave \sphinxhyphen{} interacting with an \sphinxstylestrong{environment}, and maximizing some performance to efficiently perform required tasks. Applications of RL includes \sphinxstylestrong{planning} and \sphinxstylestrong{control}.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Goals and methodology.} ML is mainly a engineering\sphinxhyphen{}oriented and an application\sphinxhyphen{}focused discipline, relying on statistical inference (\sphinxstylestrong{todo} \sphinxstyleemphasis{be more explicit}). A {\hyperref[\detokenize{ch/ml/models:ml-models}]{\sphinxcrossref{\DUrole{std,std-ref}{ML model}}}} usually takes an input \(\mathbf{u}\), and produces an output \(\mathbf{y}\), depending on its own structure and a set of parameters \(\boldsymbol{\theta}\) and hyper\sphinxhyphen{}parameters \(\boldsymbol{\mu}\). Learning usually relies on \sphinxhref{https://basics2022.github.io/bbooks-math-miscellanea/ch/optimization/intro.html}{\sphinxstylestrong{optimization}} of an objective function
\begin{equation*}
\begin{split}L(\boldsymbol{\theta}; \boldsymbol{\mu}) \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
w.r.t. parameters \(\boldsymbol{\theta}\), whose value is learned/adjusted towards an optimal solution \(\boldsymbol{\theta}^*\) that makes \(L(\boldsymbol{\theta}^*; \boldsymbol{\mu})\) extreme. The choice of hyper\sphinxhyphen{}parameters \(\boldsymbol{\mu}\) instead influences the training process and model behavior. Optimization usually relies on gradient methods, updating the parameters in the direction of the gradient of the objective function w.r.t. the parameters,
\begin{equation*}
\begin{split}\boldsymbol{\theta} \ \leftarrow \ \boldsymbol{\theta} + \alpha \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}; \boldsymbol{\mu}) \ .\end{split}
\end{equation*}


\sphinxAtStartPar
Optimization of model parameters is made fast by the use of \sphinxstylestrong{back\sphinxhyphen{}propagation} and \sphinxstylestrong{automatic differentiation} (AD), which efficiently compute gradients of the cost function with respect to the model’s parameters, and technically feasible for large\sphinxhyphen{}dimensional models \sphinxhyphen{} as the ones used in multi\sphinxhyphen{}layered neural networks, in deep learning%
\begin{footnote}[2]\sphinxAtStartFootnote
Deep learning can be roughly defined as that branch of machine learning using multi\sphinxhyphen{}layered neural networks, indeed.
%
\end{footnote} \sphinxhyphen{} by recent hardware improvement. These algorithms are not only feasible but also particularly well\sphinxhyphen{}suited (being a major driver for new designs) to modern processing architectures, such as \sphinxstylestrong{GPUs} and \sphinxstylestrong{TPUs}, that accelerate the large\sphinxhyphen{}scale matrix and tensor computations involved in both the forward and backward passes of training.

\sphinxAtStartPar
\sphinxstylestrong{todo} \sphinxstyleemphasis{Show NVIDIA, TSMC revenues}






\bigskip\hrule\bigskip


\sphinxAtStartPar
\sphinxstylestrong{todo} Add references: Bishop,…


\bigskip\hrule\bigskip


\sphinxstepscope


\section{Models in Machine Learning}
\label{\detokenize{ch/ml/models:models-in-machine-learning}}\label{\detokenize{ch/ml/models:ml-models}}\label{\detokenize{ch/ml/models::doc}}
\sphinxAtStartPar
\sphinxstylestrong{Linear models}

\sphinxAtStartPar
\sphinxstylestrong{Kernel methods}

\sphinxAtStartPar
\sphinxstylestrong{Decision trees and ensembles}

\sphinxAtStartPar
\sphinxstylestrong{Neural networks}

\sphinxAtStartPar
…probabilitstic models, clustering models, dimensionality reduction models,…

\sphinxAtStartPar
\sphinxstylestrong{Reinforcement learning models}: Q\sphinxhyphen{}learning (tabular, and DQN), Policy gradient, Actor\sphinxhyphen{}Critic, Proximal Policy Optimization,…

\sphinxstepscope


\section{Good Practices in Machine Learning}
\label{\detokenize{ch/ml/good-practice:good-practices-in-machine-learning}}\label{\detokenize{ch/ml/good-practice:ml-good-practices}}\label{\detokenize{ch/ml/good-practice::doc}}
\sphinxstepscope


\chapter{Supervised Learning}
\label{\detokenize{ch/ml/sl:supervised-learning}}\label{\detokenize{ch/ml/sl:ml-sl}}\label{\detokenize{ch/ml/sl::doc}}
\sphinxAtStartPar
\sphinxstylestrong{{\hyperref[\detokenize{ch/ml/sl-theory:ml-sl-theory}]{\sphinxcrossref{\DUrole{std,std-ref}{Theory}}}}.}

\sphinxAtStartPar
\sphinxstylestrong{Examples.}

\sphinxstepscope


\section{SL: theory}
\label{\detokenize{ch/ml/sl-theory:sl-theory}}\label{\detokenize{ch/ml/sl-theory:ml-sl-theory}}\label{\detokenize{ch/ml/sl-theory::doc}}
\sphinxAtStartPar
Supervised learning can be thought as a \sphinxstylestrong{function approximation problem}. Given a set of data
\begin{equation*}
\begin{split}\left\{ (x_i, y_i) \right\}_{i=1:N} \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
supervised learning can be formulated as the evaluation of a function \(\hat{y}(x, \boldsymbol{\theta})\), or a \sphinxstylestrong{model}, that \sphinxstyleemphasis{approximates well} the relation between input \(x_i\) and output \(y_i\),
\begin{equation*}
\begin{split}y_i \simeq \hat{y}\left( x_i; \boldsymbol{\theta} \right) \ .\end{split}
\end{equation*}
\sphinxAtStartPar
Two main tasks of SL can be distinguished on the output of the function: \sphinxstylestrong{regression} can be formulated as function approximation with continuous output, while in \sphinxstylestrong{classification} the function maps inputs to discrete output/\sphinxstylestrong{labels}

\sphinxAtStartPar
\sphinxstylestrong{Learning process} aims at finding values of the parameters \(\boldsymbol{\theta}\) (and hyper\sphinxhyphen{}parameters \(\boldsymbol{\mu}\)), that minimize a “prediction” error function, e.g. for a scalar output function,
\begin{equation*}
\begin{split}E(\boldsymbol{\theta}) = \dfrac{1}{2} \sum_{i \in D_{Tr}} | \hat{y}(x_i; \boldsymbol{\theta}) - y_i |^2 \ ,\end{split}
\end{equation*}
\sphinxAtStartPar
being \(D_{Tr}\) the set of indices belonging to the \sphinxstyleemphasis{training set}. Minimization usually relies on gradient methods of the error function w.r.t. the parameters \(\boldsymbol{\theta}\),
\begin{equation*}
\begin{split}\begin{aligned}
  \nabla_{\boldsymbol{\theta}} E(\boldsymbol{\theta}, \mathbf{x}_{Tr}, \mathbf{y}_{Tr}) & = \sum_{i \in D_{Tr}} \left( \hat{y}(x_i; \boldsymbol{\theta}) - y_i \right) \nabla_{\boldsymbol{\theta}} \hat{y}(x_i; \boldsymbol{\theta}) \\
  \boldsymbol{\theta} & \leftarrow \boldsymbol{\theta} - \alpha \nabla_{\boldsymbol{\theta}} E(\boldsymbol{\theta}, \mathbf{x}_{Tr}, \mathbf{y}_{Tr}) \ ,
\end{aligned}\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\alpha\) an hyper\sphinxhyphen{}parameter called \sphinxstyleemphasis{learning rate}, governing the “length” of the update step.
Other objective functions to be maximised or minimized can be used. Slight variations to objective functions allow for regularization (e.g. parameter weighting)

\sphinxAtStartPar
\sphinxstylestrong{Dataset.} Available data \(\{ x_i, y_i \}_i\) is divided in different sets:
\begin{itemize}
\item {} 
\sphinxAtStartPar
training set: for learning/tuning model parameters, minimizing an error function

\item {} 
\sphinxAtStartPar
validation set: for early stopping, and hyper\sphinxhyphen{}parameter tuning (e.g. to avoid

\item {} 
\sphinxAtStartPar
test set: to evaluate model performance

\end{itemize}

\sphinxstepscope


\chapter{Unsupervised Learning}
\label{\detokenize{ch/ml/ul:unsupervised-learning}}\label{\detokenize{ch/ml/ul:ml-ul}}\label{\detokenize{ch/ml/ul::doc}}
\sphinxstepscope


\chapter{Reinforcement Learning}
\label{\detokenize{ch/ml/rl:reinforcement-learning}}\label{\detokenize{ch/ml/rl:ml-rl}}\label{\detokenize{ch/ml/rl::doc}}





\renewcommand{\indexname}{Proof Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{def:measurable\sphinxhyphen{}space}
\item\relax\sphinxstyleindexentry{def:measurable\sphinxhyphen{}space}\sphinxstyleindexextra{ch/prob/rv\sphinxhyphen{}def}\sphinxstyleindexpageref{ch/prob/rv-def:\detokenize{def:measurable-space}}
\indexspace
\bigletter{def:prob\sphinxhyphen{}measure}
\item\relax\sphinxstyleindexentry{def:prob\sphinxhyphen{}measure}\sphinxstyleindexextra{ch/prob/rv\sphinxhyphen{}def}\sphinxstyleindexpageref{ch/prob/rv-def:\detokenize{def:prob-measure}}
\indexspace
\bigletter{def:prob\sphinxhyphen{}space}
\item\relax\sphinxstyleindexentry{def:prob\sphinxhyphen{}space}\sphinxstyleindexextra{ch/prob/rv\sphinxhyphen{}def}\sphinxstyleindexpageref{ch/prob/rv-def:\detokenize{def:prob-space}}
\indexspace
\bigletter{def:rv}
\item\relax\sphinxstyleindexentry{def:rv}\sphinxstyleindexextra{ch/prob/rv\sphinxhyphen{}def}\sphinxstyleindexpageref{ch/prob/rv-def:\detokenize{def:rv}}
\indexspace
\bigletter{def:sigma\sphinxhyphen{}algebra}
\item\relax\sphinxstyleindexentry{def:sigma\sphinxhyphen{}algebra}\sphinxstyleindexextra{ch/prob/rv\sphinxhyphen{}def}\sphinxstyleindexpageref{ch/prob/rv-def:\detokenize{def:sigma-algebra}}
\indexspace
\bigletter{definition\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}0}\sphinxstyleindexextra{ch/prob/iid}\sphinxstyleindexpageref{ch/prob/iid:\detokenize{definition-0}}
\indexspace
\bigletter{definition\sphinxhyphen{}2}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}2}\sphinxstyleindexextra{ch/prob/rv\sphinxhyphen{}multi\sphinxhyphen{}dimensional}\sphinxstyleindexpageref{ch/prob/rv-multi-dimensional:\detokenize{definition-2}}
\indexspace
\bigletter{ex:char\sphinxhyphen{}fun:independent}
\item\relax\sphinxstyleindexentry{ex:char\sphinxhyphen{}fun:independent}\sphinxstyleindexextra{ch/prob/characteristic\sphinxhyphen{}fun}\sphinxstyleindexpageref{ch/prob/characteristic-fun:\detokenize{ex:char-fun:independent}}
\indexspace
\bigletter{ex:char\sphinxhyphen{}fun:multidimensional}
\item\relax\sphinxstyleindexentry{ex:char\sphinxhyphen{}fun:multidimensional}\sphinxstyleindexextra{ch/prob/characteristic\sphinxhyphen{}fun}\sphinxstyleindexpageref{ch/prob/characteristic-fun:\detokenize{ex:char-fun:multidimensional}}
\indexspace
\bigletter{ex:char\sphinxhyphen{}fun:normal}
\item\relax\sphinxstyleindexentry{ex:char\sphinxhyphen{}fun:normal}\sphinxstyleindexextra{ch/prob/characteristic\sphinxhyphen{}fun}\sphinxstyleindexpageref{ch/prob/characteristic-fun:\detokenize{ex:char-fun:normal}}
\indexspace
\bigletter{ex:char\sphinxhyphen{}fun:taylor}
\item\relax\sphinxstyleindexentry{ex:char\sphinxhyphen{}fun:taylor}\sphinxstyleindexextra{ch/prob/characteristic\sphinxhyphen{}fun}\sphinxstyleindexpageref{ch/prob/characteristic-fun:\detokenize{ex:char-fun:taylor}}
\indexspace
\bigletter{example\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}0}\sphinxstyleindexextra{ch/prob/processes\sphinxhyphen{}calculus}\sphinxstyleindexpageref{ch/prob/processes-calculus:\detokenize{example-0}}
\indexspace
\bigletter{example:gbm\sphinxhyphen{}drift:finance}
\item\relax\sphinxstyleindexentry{example:gbm\sphinxhyphen{}drift:finance}\sphinxstyleindexextra{ch/prob/processes\sphinxhyphen{}calculus}\sphinxstyleindexpageref{ch/prob/processes-calculus:\detokenize{example:gbm-drift:finance}}
\indexspace
\bigletter{example:gbm:finance}
\item\relax\sphinxstyleindexentry{example:gbm:finance}\sphinxstyleindexextra{ch/prob/processes\sphinxhyphen{}calculus}\sphinxstyleindexpageref{ch/prob/processes-calculus:\detokenize{example:gbm:finance}}
\indexspace
\bigletter{property\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}0}\sphinxstyleindexextra{ch/prob/rv\sphinxhyphen{}multi\sphinxhyphen{}dimensional}\sphinxstyleindexpageref{ch/prob/rv-multi-dimensional:\detokenize{property-0}}
\indexspace
\bigletter{property\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}1}\sphinxstyleindexextra{ch/prob/wiener}\sphinxstyleindexpageref{ch/prob/wiener:\detokenize{property-1}}
\indexspace
\bigletter{property\sphinxhyphen{}2}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}2}\sphinxstyleindexextra{ch/prob/wiener}\sphinxstyleindexpageref{ch/prob/wiener:\detokenize{property-2}}
\indexspace
\bigletter{property\sphinxhyphen{}3}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}3}\sphinxstyleindexextra{ch/prob/wiener}\sphinxstyleindexpageref{ch/prob/wiener:\detokenize{property-3}}
\indexspace
\bigletter{theorem\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}1}\sphinxstyleindexextra{ch/prob/rv\sphinxhyphen{}multi\sphinxhyphen{}dimensional}\sphinxstyleindexpageref{ch/prob/rv-multi-dimensional:\detokenize{theorem-1}}
\indexspace
\bigletter{thm:clt}
\item\relax\sphinxstyleindexentry{thm:clt}\sphinxstyleindexextra{ch/prob/iid}\sphinxstyleindexpageref{ch/prob/iid:\detokenize{thm:clt}}
\indexspace
\bigletter{wiener:def}
\item\relax\sphinxstyleindexentry{wiener:def}\sphinxstyleindexextra{ch/prob/wiener}\sphinxstyleindexpageref{ch/prob/wiener:\detokenize{wiener:def}}
\indexspace
\bigletter{wn:def:derivative}
\item\relax\sphinxstyleindexentry{wn:def:derivative}\sphinxstyleindexextra{ch/prob/white\sphinxhyphen{}noise}\sphinxstyleindexpageref{ch/prob/white-noise:\detokenize{wn:def:derivative}}
\indexspace
\bigletter{wn:def:properties}
\item\relax\sphinxstyleindexentry{wn:def:properties}\sphinxstyleindexextra{ch/prob/white\sphinxhyphen{}noise}\sphinxstyleindexpageref{ch/prob/white-noise:\detokenize{wn:def:properties}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}